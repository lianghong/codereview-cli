# Model Configuration for Code Review Tool
# This file defines all available AI models for code review analysis
#
# Pricing and parameters researched: January 2026
# Sources: AWS Bedrock, Azure OpenAI, Anthropic, Mistral, Moonshot, Alibaba docs

version: "1.1"

providers:
  # AWS Bedrock Provider
  bedrock:
    region: us-west-2
    models:
      # Claude Models (Anthropic)
      # Best for: Comprehensive code review, complex reasoning, security analysis
      - id: opus
        full_id: global.anthropic.claude-opus-4-6-v1
        name: Claude Opus 4.6
        aliases:
          - claude-opus
          - opus4.6
          - claude-opus-4.6
        pricing:
          input_per_million: 5.00    # Verified: Anthropic pricing
          output_per_million: 25.00
          cache_write_per_million: 6.25   # Prompt caching available
          cache_read_per_million: 0.50
        inference_params:
          default_temperature: 0.1   # Low temp for deterministic code analysis
          max_output_tokens: 128000  # Supports up to 128K output
        context_window: 200000
        capabilities:
          - code_review
          - security_analysis
          - reasoning
          - computer_use

      - id: opus4.5
        full_id: global.anthropic.claude-opus-4-5-20251101-v1:0
        name: Claude Opus 4.5
        aliases:
          - claude-opus-4.5
        pricing:
          input_per_million: 5.00    # Verified: Anthropic pricing
          output_per_million: 25.00
          cache_write_per_million: 6.25   # Prompt caching available
          cache_read_per_million: 0.50
        inference_params:
          default_temperature: 0.1   # Low temp for deterministic code analysis
          max_output_tokens: 64000   # Supports up to 64K output
        context_window: 200000
        capabilities:
          - code_review
          - security_analysis
          - reasoning
          - computer_use

      - id: sonnet
        full_id: global.anthropic.claude-sonnet-4-5-20250929-v1:0
        name: Claude Sonnet 4.5
        aliases:
          - claude-sonnet
        pricing:
          input_per_million: 3.00    # Verified: Anthropic pricing
          output_per_million: 15.00
          cache_write_per_million: 3.75
          cache_read_per_million: 0.30
        inference_params:
          default_temperature: 0.1   # Low temp for accurate code review
          max_output_tokens: 64000
        context_window: 200000
        capabilities:
          - code_review
          - coding
          - agents
          - computer_use
        notes: "77.2% SWE-bench Verified, best balance of cost/performance"

      - id: haiku
        full_id: global.anthropic.claude-haiku-4-5-20251001-v1:0
        name: Claude Haiku 4.5
        aliases:
          - claude-haiku
        pricing:
          input_per_million: 1.00    # Verified: Anthropic pricing
          output_per_million: 5.00
          cache_write_per_million: 1.25
          cache_read_per_million: 0.10
        inference_params:
          default_temperature: 0.1   # Low temp for consistent output
          max_output_tokens: 64000
        context_window: 200000
        capabilities:
          - code_review
          - real_time_assistants
          - parallel_agents
        notes: "73.3% SWE-bench Verified, fast and cost-effective"

      # Minimax M2 Model (Bedrock)
      # MoE: 230B total, 10B active parameters
      # Best for: Fast code analysis, agentic tasks
      - id: minimax-bedrock
        full_id: minimax.minimax-m2
        name: Minimax M2 (Bedrock)
        aliases:
          - mm2-bedrock
        pricing:
          input_per_million: 0.30    # Verified: AWS Bedrock pricing
          output_per_million: 1.20
        inference_params:
          default_temperature: 0.3   # Lower for code review (default is 1.0)
          default_top_p: 0.9
          default_top_k: 40
          max_output_tokens: 8192
        context_window: 256000
        architecture: "MoE (230B total, 10B active)"
        capabilities:
          - coding
          - agentic_tasks
          - general_intelligence
        notes: "Compact, fast MoE model optimized for coding tasks"

      # Mistral Large 3 Model
      # MoE: 675B total, 41B active parameters
      # Best for: Cost-effective code review, multi-language support
      - id: mistral
        full_id: mistral.mistral-large-3-675b-instruct
        name: Mistral Large 3
        aliases:
          - mistral-large
        pricing:
          input_per_million: 2.00    # Verified: ~80% cheaper than competitors
          output_per_million: 6.00
        inference_params:
          default_temperature: 0.1   # Low temp for precise code analysis
          default_top_p: 0.9
          max_output_tokens: 32000
        context_window: 128000
        architecture: "MoE (675B total, 41B active)"
        capabilities:
          - code_review
          - multi_language
          - reasoning
        notes: "Open source, cost-effective for large-scale analysis"

      # Kimi K2 Thinking Model (Moonshot AI)
      # MoE: 1T total, 32B active parameters
      # Best for: Complex reasoning, multi-step code analysis
      - id: kimi-k2-bedrock
        full_id: moonshot.kimi-k2-thinking
        name: Kimi K2 Thinking (Bedrock)
        aliases:
          - kimi
          - kimi-k2
          - kimi-bedrock
        pricing:
          input_per_million: 0.60    # Verified: litellm/Bedrock pricing
          output_per_million: 2.50
        inference_params:
          default_temperature: 0.6   # Moonshot recommended setting
          default_min_p: 0.01        # Suppress unlikely tokens
          max_output_tokens: 32000   # Can go up to 256K
        context_window: 256000
        architecture: "MoE (1T total, 32B active)"
        capabilities:
          - reasoning
          - autonomous_agents
          - tool_use
          - multi_step_tasks
        notes: "Exceptional for autonomous AI agents, maintains coherence over hundreds of tool calls"

      # Qwen3 Coder Model (Alibaba)
      # MoE: 480B total, 35B active parameters
      # Best for: Specialized code generation and review
      - id: qwen-bedrock
        full_id: qwen.qwen3-coder-480b-a35b-v1:0
        name: Qwen3 Coder 480B (Bedrock)
        aliases:
          - qwen
          - qwen-coder
        pricing:
          input_per_million: 0.22    # Verified: AWS Bedrock pricing
          output_per_million: 1.40
        inference_params:
          default_temperature: 0.3   # Lower than default 0.7 for code review
          default_top_p: 0.8         # Qwen recommended
          default_top_k: 20          # Qwen recommended
          repetition_penalty: 1.05   # Qwen recommended
          max_output_tokens: 65536
        context_window: 131072
        architecture: "MoE (480B total, 35B active)"
        capabilities:
          - code_generation
          - code_review
          - agentic_coding
          - browser_automation
        notes: "87.9% MultiPL-E score, specialized for coding tasks"

      # DeepSeek-R1 Model (DeepSeek AI)
      # Reasoning model with chain-of-thought capabilities
      # Best for: Complex reasoning, code analysis with thinking
      # Note: Does not support tool use - uses prompt-based JSON parsing
      - id: deepseek-r1-bedrock
        full_id: us.deepseek.r1-v1:0
        name: DeepSeek-R1 (Bedrock)
        aliases:
          - deepseek
          - deepseek-r1
          - ds-bedrock
          - deepseek-bedrock
        pricing:
          input_per_million: 1.35    # AWS Bedrock pricing (TBD - update when available)
          output_per_million: 5.40
        inference_params:
          default_temperature: 0.6   # DeepSeek recommended (0.5-0.7 range)
          max_output_tokens: 32000   # Note: top_k/top_p may not be supported on Bedrock
        context_window: 128000       # 128K token context window
        supports_tool_use: false     # DeepSeek-R1 doesn't support tool calling
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - thinking
        notes: "Reasoning model with chain-of-thought, excellent for complex code analysis"

  # Azure OpenAI Provider
  azure_openai:
    endpoint: "${AZURE_OPENAI_ENDPOINT}"
    api_key: "${AZURE_OPENAI_API_KEY}"
    api_version: "2025-04-01-preview"
    models:
      # GPT-5.2 Codex Model
      # Best for: Enterprise code review, security-aware analysis
      # Note: Requires Responses API (doesn't support ChatCompletion API)
      - id: gpt-5.2-codex
        deployment_name: gpt-5.2-codex
        name: GPT-5.2 Codex
        aliases:
          - gpt
          - gpt52
          - codex
        pricing:
          input_per_million: 1.75    # Verified: Microsoft Foundry pricing
          output_per_million: 14.00
          cached_input_per_million: 0.175  # 90% discount on cached
        inference_params:
          default_temperature: 0.0   # Zero temp for deterministic code analysis
          default_top_p: 0.95
          max_output_tokens: 128000
        use_responses_api: true      # Required: GPT-5.2 Codex uses Responses API, not ChatCompletion
        context_window: 400000       # 272K input, 128K output
        capabilities:
          - reasoning
          - code_review
          - security_analysis
          - structured_output
          - function_calling
        notes: "56.4% SWE-Bench Pro, security-aware by design"

      # Kimi K2.5 (via Azure AI Foundry) - Moonshot AI's multimodal MoE model
      # MoE: 1T total, 32B active parameters
      # Best for: Complex reasoning, multimodal code analysis, agentic workflows
      - id: kimi-k2.5-azure
        deployment_name: Kimi-K2.5
        name: Kimi K2.5 (Azure)
        aliases:
          - kimi25-azure
          - kimi-azure
        pricing:
          input_per_million: 0.60    # Microsoft Foundry pricing (Feb 2026)
          output_per_million: 3.00
        inference_params:
          default_temperature: 0.6   # Moonshot recommended for thinking mode
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 262144       # 256K token context window
        capabilities:
          - reasoning
          - multimodal
          - code_review
          - tool_use
          - agentic_tasks
        notes: "1T MoE multimodal model, 96.1% AIME 2025, Agent Swarm support"

      # Grok 4 Fast Reasoning (via Azure AI Foundry) - xAI's cost-efficient reasoning model
      # Best for: Fast reasoning, cost-efficient code analysis, long-context review
      - id: grok-4-fast
        deployment_name: grok-4-fast-reasoning
        name: Grok 4 Fast Reasoning (Azure)
        aliases:
          - grok
          - grok4
          - grok-fast
          - g4fast
        pricing:
          input_per_million: 0.20    # xAI/Azure pricing
          output_per_million: 0.50
        inference_params:
          default_temperature: 0.1   # Low temp for deterministic code analysis
          default_top_p: 0.95
          max_output_tokens: 32000
        context_window: 2000000      # 2M token context window
        capabilities:
          - reasoning
          - code_review
          - structured_output
          - function_calling
        notes: "2M context, SOTA cost-efficiency, 212 tokens/sec"

  # NVIDIA NIM API Provider
  # Models available via build.nvidia.com
  nvidia:
    api_key: "${NVIDIA_API_KEY}"
    # base_url: "${NVIDIA_BASE_URL}"  # Optional: for self-hosted NIMs
    polling_timeout: 900  # 15 minutes for 202 polling (large models may need time)
    max_retries: 5        # Retry 504/502/503 gateway errors up to 5 times
    models:
      # Devstral 2 123B - Mistral's code-specialized model
      # Best for: Code review, debugging, code generation
      # 72.2% SWE-Bench Verified score
      - id: devstral
        full_id: mistralai/devstral-2-123b-instruct-2512
        name: Devstral 2 123B
        aliases:
          - devstral-2
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.15  # Low for deterministic code analysis
          default_top_p: 0.95
          max_output_tokens: 8192
        context_window: 128000
        capabilities:
          - code_review
          - code_generation
          - debugging
          - agentic_coding
        notes: "72.2% SWE-Bench Verified, excellent for code review tasks"

      # MiniMax M2 (via NVIDIA) - Fast reasoning model
      # Best for: Quick code analysis, general reasoning
      - id: minimax-nvidia
        full_id: minimaxai/minimax-m2
        name: MiniMax M2 (NVIDIA)
        aliases:
          - mm2-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.3   # Lowered for code review (MiniMax default is 1.0)
          default_top_p: 0.9
          max_output_tokens: 8192
        context_window: 256000
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
        notes: "Fast MoE model, good for quick code analysis"

      # MiniMax M2.1 (via NVIDIA) - Enhanced reasoning model
      # 200K context window, 128K max output
      # Best for: Code review, coding tasks, agentic workflows
      - id: minimax-m2.1-nvidia
        full_id: minimaxai/minimax-m2.1
        name: MiniMax M2.1 (NVIDIA)
        aliases:
          - mm2.1-nvidia
          - minimax-m2.1
          - mm21
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 1.0   # MiniMax recommended for coding/agentic workflows
          default_top_p: 0.95        # MiniMax recommended
          default_top_k: 40          # MiniMax recommended
          max_output_tokens: 128000  # Supports up to 128K output
          enable_thinking: true      # Model produces <think> reasoning tags
          clear_thinking: false      # Preserve reasoning context
        context_window: 200000
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
          - code_review
          - thinking
        notes: "Enhanced MoE model with 200K context and 128K output, optimized for coding and agentic workflows"

      # Qwen3 Coder 480B (via NVIDIA) - Specialized coding model
      # MoE: 480B total, 35B active parameters
      # Best for: Code generation, code review, agentic coding
      - id: qwen-nvidia
        full_id: qwen/qwen3-coder-480b-a35b-instruct
        name: Qwen3 Coder 480B (NVIDIA)
        aliases:
          - qwen3-nvidia
          - qwen-coder-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.3   # Lowered for code review (matches Bedrock Qwen)
          default_top_p: 0.8         # Qwen recommended
          default_top_k: 20          # Qwen recommended
          max_output_tokens: 16384   # Increased for detailed code review reports
          enable_thinking: true      # Enable thinking mode for deeper code analysis
          clear_thinking: false      # Preserve reasoning context
        context_window: 131072
        architecture: "MoE (480B total, 35B active)"
        capabilities:
          - code_generation
          - code_review
          - agentic_coding
          - thinking
        notes: "87.9% MultiPL-E score, specialized for coding tasks via NVIDIA NIM"

      # Kimi K2 Instruct (via NVIDIA) - Reasoning model from Moonshot AI
      # MoE: 1T total, 32B active parameters
      # Best for: Complex reasoning, multi-step code analysis
      - id: kimi-k2-nvidia
        full_id: moonshotai/kimi-k2-instruct-0905
        name: Kimi K2 Instruct (NVIDIA)
        aliases:
          - kimi-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.5   # Lowered for code review (default is 1.0)
          default_top_p: 0.9
          default_min_p: 0.01        # Suppress unlikely tokens (Moonshot recommended)
          max_output_tokens: 16384   # Increased for detailed reports
        context_window: 256000
        architecture: "MoE (1T total, 32B active)"
        capabilities:
          - reasoning
          - autonomous_agents
          - tool_use
          - multi_step_tasks
        notes: "Exceptional for autonomous AI agents, maintains coherence over hundreds of tool calls"

      # DeepSeek V3.2 (via NVIDIA) - Advanced reasoning model
      # MoE: 671B total, 37B active parameters
      # Best for: Complex reasoning, code analysis with thinking
      - id: deepseek-v3.2-nvidia
        full_id: deepseek-ai/deepseek-v3.2
        name: DeepSeek V3.2 (NVIDIA)
        aliases:
          - deepseek-v3-nvidia
          - ds-nvidia
          - deepseek-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.3   # Lowered for code review (default is 1.0)
          default_top_p: 0.9
          max_output_tokens: 16384   # Increased for detailed reports
          enable_thinking: true      # Enable deep reasoning for complex code analysis
          clear_thinking: false      # Preserve thinking across the conversation
        context_window: 128000
        architecture: "MoE (671B total, 37B active)"
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - thinking
        notes: "Advanced reasoning with thinking capability, excellent for complex code analysis"

      # Kimi K2.5 (via NVIDIA) - Multimodal MoE model from Moonshot AI
      # MoE: 1T total, multimodal (text, image, video)
      # Best for: Complex reasoning, multimodal code analysis
      - id: kimi-k2.5-nvidia
        full_id: moonshotai/kimi-k2.5
        name: Kimi K2.5 (NVIDIA)
        aliases:
          - kimi-k2.5
          - kimi25
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.6   # Moonshot recommended for Instant mode (instructed tasks)
          default_top_p: 0.95        # Moonshot recommended
          default_top_k: 40          # Moonshot recommended
          max_output_tokens: 16384   # Supports up to 32K, using 16K for detailed reports
        context_window: 256000       # 256K token context window
        architecture: "MoE (1T total, multimodal)"
        capabilities:
          - reasoning
          - multimodal
          - code_review
          - tool_use
        notes: "1T multimodal MoE for high-capacity video and image understanding with efficient inference"

      # GLM4.7 (via NVIDIA) - Zhipu AI's reasoning model
      # 73.8% SWE-bench score, supports interleaved thinking
      # Best for: Complex reasoning, code analysis with thinking
      - id: glm47
        full_id: z-ai/glm4.7
        name: GLM 4.7 (NVIDIA)
        aliases:
          - glm4
          - glm-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.5   # Balance between creativity and determinism (default 1.0, SWE-bench uses 0.7)
          default_top_p: 0.95        # Official recommendation
          max_output_tokens: 16384   # SWE-bench setting, sufficient for detailed review
          enable_thinking: true      # Enable interleaved thinking for complex reasoning
          clear_thinking: false      # Preserve thinking across the conversation
        context_window: 200000       # Supports up to 200K tokens
        capabilities:
          - reasoning
          - code_review
          - thinking
          - tool_use
        notes: "73.8% SWE-bench, interleaved thinking mode enabled for complex code analysis"

      # GLM-5 (via NVIDIA) - Zhipu AI's next-generation reasoning model
      # 200K context window, 128K max output
      # Best for: Complex reasoning, code analysis, general intelligence
      - id: glm5
        full_id: z-ai/glm5
        name: GLM-5 (NVIDIA)
        aliases:
          - glm-5
          - glm5-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 1.0   # Z.ai recommended default
          default_top_p: 0.95        # Z.ai recommended default
          max_output_tokens: 128000  # Supports up to 128K output
        context_window: 200000       # 200K token context window
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - general_intelligence
        notes: "Next-gen GLM model, 200K context with 128K output. Lower temp (0.5-0.7) recommended for complex non-creative prompts."

      # Step-3.5-Flash (via NVIDIA) - StepFun's cost-efficient reasoning model
      # 256K context window, optimized for fast inference
      # Best for: General code review, chat-style analysis, agentic tasks
      - id: step-3.5-flash
        full_id: stepfun-ai/step-3.5-flash
        name: Step 3.5 Flash (NVIDIA)
        aliases:
          - step-flash
          - step35
          - step-nvidia
        pricing:
          input_per_million: 0.00    # Currently free tier
          output_per_million: 0.00   # Update when NVIDIA announces pricing
        inference_params:
          default_temperature: 0.6   # Recommended for coherent code analysis (range 0.6-1.0)
          default_top_p: 0.95        # NVIDIA NIM recommended
          default_top_k: 40          # NVIDIA NIM recommended
          max_output_tokens: 16384   # Sufficient for detailed code review reports
        context_window: 256000       # 256K token context window
        capabilities:
          - code_review
          - reasoning
          - general_intelligence
        notes: "Cost-efficient 256K context model, good coherence at temperature 0.6"

  # Google Generative AI Provider
  # Models available via Google AI Studio / Gemini API
  google_genai:
    api_key: "${GOOGLE_API_KEY}"
    models:
      # Gemini 3 Pro Preview - Google's flagship reasoning model
      # 1M token context, strong reasoning and code analysis
      - id: gemini-3-pro
        full_id: gemini-3-pro-preview
        name: Gemini 3 Pro Preview
        aliases:
          - gemini-pro
          - gemini3-pro
          - g3pro
        pricing:
          input_per_million: 2.00
          output_per_million: 12.00
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 1000000
        capabilities:
          - code_review
          - reasoning
          - multimodal
          - tool_use

      # Gemini 3 Flash Preview - Fast and cost-efficient
      # 1M token context, Flash-tier pricing with Pro-level quality
      - id: gemini-3-flash
        full_id: gemini-3-flash-preview
        name: Gemini 3 Flash Preview
        aliases:
          - gemini-flash
          - gemini3-flash
          - g3flash
        pricing:
          input_per_million: 0.50
          output_per_million: 3.00
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 1000000
        capabilities:
          - code_review
          - reasoning
          - multimodal

# Default model configuration
defaults:
  bedrock_default: opus
  azure_default: gpt-5.2-codex
  nvidia_default: devstral
  aws_region: us-west-2
  max_tokens: 16000

# File scanning configuration
scanning:
  max_file_size_kb: 500
  warn_file_size_kb: 100
  exclude_patterns:
    # Dependency directories
    - "**/node_modules/**"
    - "**/.venv/**"
    - "**/venv/**"
    - "**/vendor/**"
    # Build outputs
    - "**/dist/**"
    - "**/build/**"
    - "**/__pycache__/**"
    - "**/*.pyc"
    - "**/*.pyo"
    # Version control
    - "**/.git/**"
    - "**/.svn/**"
    # IDE and editors
    - "**/.vscode/**"
    - "**/.idea/**"
    - "**/*.swp"
    # Test fixtures and generated code
    - "**/test_data/**"
    - "**/fixtures/**"
    - "**/*_pb2.py"
    - "**/*_pb2_grpc.py"
    # Common non-review files
    - "**/*.min.js"
    - "**/*.min.css"
    - "**/migrations/**"
    - "**/*.lock"
  exclude_extensions:
    - ".json"
    - ".yaml"
    - ".yml"
    - ".toml"
    - ".md"
    - ".txt"
    - ".rst"
    - ".jpg"
    - ".png"
    - ".gif"
    - ".svg"
    - ".bin"
    - ".exe"
    - ".so"
    - ".dylib"
    - ".pyc"
    - ".pyo"

# Code Review Specific Recommendations
#
# For accurate, deterministic code review:
# - Use temperature 0.0-0.3 (lower = more consistent)
# - Use top_p 0.8-0.95 for controlled output
# - Prefer models with strong SWE-bench scores
#
# Model selection by use case:
# - Quick reviews: Haiku ($1/$5) or Qwen ($0.22/$1.40)
# - Standard reviews: Sonnet ($3/$15) or Mistral ($2/$6)
# - Complex/security reviews: Opus ($5/$25) or GPT-5.2-Codex ($1.75/$14)
# - Budget-conscious: Qwen ($0.22/$1.40) or Minimax ($0.30/$1.20)
