# Model Configuration for Code Review Tool
# This file defines all available AI models
# for code review analysis
#
# Pricing/parameters researched: January 2026
# Sources: AWS Bedrock, Azure OpenAI,
#   Anthropic, Moonshot, Alibaba, Zhipu

---
version: "1.1"

providers:
  # AWS Bedrock Provider
  bedrock:
    region: us-west-2
    models:
      # Claude Models (Anthropic)
      # Best for: Code review, reasoning, security
      - id: opus
        full_id: global.anthropic.claude-opus-4-6-v1
        name: Claude Opus 4.6
        aliases:
          - claude-opus
          - opus4.6
          - claude-opus-4.6
        pricing:
          input_per_million: 5.00
          output_per_million: 25.00
          cache_write_per_million: 6.25
          cache_read_per_million: 0.50
        inference_params:
          # Low temp for deterministic code analysis
          default_temperature: 0.1
          max_output_tokens: 128000
        context_window: 200000
        capabilities:
          - code_review
          - security_analysis
          - reasoning
          - computer_use

      - id: sonnet
        full_id: global.anthropic.claude-sonnet-4-6
        name: Claude Sonnet 4.6
        aliases:
          - claude-sonnet
          - sonnet4.6
          - claude-sonnet-4.6
        pricing:
          input_per_million: 3.00
          output_per_million: 15.00
          cache_write_per_million: 3.75
          cache_read_per_million: 0.30
        inference_params:
          default_temperature: 0.1
          max_output_tokens: 64000
        context_window: 200000
        capabilities:
          - code_review
          - coding
          - agents
          - computer_use
          - reasoning
        notes: >-
          Approaches Opus 4.6 intelligence at
          lower cost, frontier coding and agents

      - id: haiku
        full_id: >-
          global.anthropic.claude-haiku-4-5-20251001-v1:0
        name: Claude Haiku 4.5
        aliases:
          - claude-haiku
        pricing:
          input_per_million: 1.00
          output_per_million: 5.00
          cache_write_per_million: 1.25
          cache_read_per_million: 0.10
        inference_params:
          default_temperature: 0.1
          max_output_tokens: 64000
        context_window: 200000
        capabilities:
          - code_review
          - real_time_assistants
          - parallel_agents
        notes: >-
          73.3% SWE-bench Verified, fast and
          cost-effective

      # Qwen3 Coder Model (Alibaba)
      # MoE: 480B total, 35B active parameters
      # Best for: Specialized code generation
      - id: qwen-bedrock
        full_id: qwen.qwen3-coder-480b-a35b-v1:0
        name: Qwen3 Coder 480B (Bedrock)
        aliases:
          - qwen
          - qwen-coder
        pricing:
          input_per_million: 0.22
          output_per_million: 1.40
        inference_params:
          # Lower than default 0.7 for code review
          default_temperature: 0.3
          default_top_p: 0.8
          default_top_k: 20
          max_output_tokens: 65536
        context_window: 131072
        architecture: "MoE (480B total, 35B active)"
        capabilities:
          - code_generation
          - code_review
          - agentic_coding
          - browser_automation
        notes: >-
          87.9% MultiPL-E score, specialized
          for coding tasks

      # DeepSeek-R1 Model (DeepSeek AI)
      # Reasoning model with chain-of-thought
      # Best for: Complex reasoning, code analysis
      # Note: No tool use - prompt-based JSON
      - id: deepseek-r1-bedrock
        full_id: us.deepseek.r1-v1:0
        name: DeepSeek-R1 (Bedrock)
        aliases:
          - deepseek
          - deepseek-r1
          - ds-bedrock
          - deepseek-bedrock
        pricing:
          # TBD - update when available
          input_per_million: 1.35
          output_per_million: 5.40
        inference_params:
          # Lower end of DeepSeek range (0.5-0.7)
          default_temperature: 0.5
          # top_k/top_p unsupported on Bedrock
          max_output_tokens: 32000
        context_window: 128000
        supports_tool_use: false
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - thinking
        notes: >-
          Reasoning with chain-of-thought,
          excellent for complex code analysis

      # DeepSeek V3.2 Model (via Bedrock)
      # MoE: 671B total, 37B active parameters
      # Best for: Agentic workflows, tool calling
      - id: deepseek-v3.2-bedrock
        full_id: deepseek.v3.2
        name: DeepSeek V3.2 (Bedrock)
        aliases:
          - deepseek-v3-bedrock
          - ds-v3-bedrock
        pricing:
          # Verified: AWS Bedrock (Feb 2026)
          input_per_million: 0.62
          output_per_million: 1.85
        inference_params:
          # Lowered for code review (default 1.0)
          default_temperature: 0.3
          default_top_p: 0.9
          max_output_tokens: 16384
        context_window: 164000
        architecture: "MoE (671B total, 37B active)"
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - agentic_tasks
          - tool_use
        notes: >-
          Sparse MoE, excels at agentic workflows
          and multi-step tool calling

      # MiniMax M2.1 Model (Bedrock)
      # Best for: Multilingual, autonomous coding
      - id: minimax-m2.1-bedrock
        full_id: minimax.minimax-m2.1
        name: MiniMax M2.1 (Bedrock)
        aliases:
          - mm2.1-bedrock
        pricing:
          # Verified: AWS Bedrock (Feb 2026)
          input_per_million: 0.30
          output_per_million: 1.20
        inference_params:
          # Lowered for code review (Bedrock has
          # no thinking mode support)
          default_temperature: 0.3
          default_top_p: 0.95
          default_top_k: 40
          max_output_tokens: 128000
        context_window: 196000
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
          - code_review
          - thinking
        notes: >-
          Enhanced MoE with 196K context and
          128K output, multilingual coding

      # GLM 4.7 Model (Zhipu AI via Bedrock)
      # 73.8% SWE-bench, interleaved thinking
      # Best for: Complex reasoning with thinking
      - id: glm47-bedrock
        full_id: zai.glm-4.7
        name: GLM 4.7 (Bedrock)
        aliases:
          - glm4-bedrock
        pricing:
          # TBD - update when AWS publishes
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Low temp for code review (Bedrock has
          # no thinking mode support)
          default_temperature: 0.3
          default_top_p: 0.95
          max_output_tokens: 16384
        context_window: 200000
        capabilities:
          - reasoning
          - code_review
          - thinking
          - tool_use
        notes: >-
          73.8% SWE-bench, interleaved thinking,
          Bedrock

      # GLM 4.7 Flash Model (Zhipu AI, Bedrock)
      # MoE: 30B total, 3B active parameters
      # Best for: Fast code review, cost-efficient
      - id: glm47-flash-bedrock
        full_id: zai.glm-4.7-flash
        name: GLM 4.7 Flash (Bedrock)
        aliases:
          - glm4-flash
          - glm47f
          - glm47-flash
        pricing:
          # TBD - update when AWS publishes
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Low temp for code review
          default_temperature: 0.3
          default_top_p: 0.95
          max_output_tokens: 16384
        context_window: 200000
        architecture: "MoE (30B total, 3B active)"
        capabilities:
          - code_review
          - code_generation
          - reasoning
        notes: >-
          Lightweight MoE, cost-efficient
          for production deployment

      # Kimi K2.5 Model (Moonshot AI, Bedrock)
      # MoE: 1T total, 32B active parameters
      # Best for: Reasoning, multimodal analysis
      - id: kimi-k2.5-bedrock
        full_id: moonshotai.kimi-k2.5
        name: Kimi K2.5 (Bedrock)
        aliases:
          - kimi
          - kimi-bedrock
          - kimi25-bedrock
        pricing:
          # Verified: AWS Bedrock (Feb 2026)
          input_per_million: 0.60
          output_per_million: 3.00
        inference_params:
          # Lowered from 0.6 for code review
          default_temperature: 0.5
          default_top_p: 0.95
          default_top_k: 40
          max_output_tokens: 65536
        context_window: 262000
        architecture: "MoE (1T total, 32B active)"
        capabilities:
          - reasoning
          - multimodal
          - code_review
          - tool_use
          - agentic_tasks
        notes: >-
          1T MoE multimodal, native image
          understanding, Bedrock Converse API

      # Qwen3 Coder Next (Alibaba, Bedrock)
      # MoE: 80B total, 3B active parameters
      # Best for: Efficient agentic coding
      # Note: Does NOT support thinking mode
      - id: qwen-next-bedrock
        full_id: qwen.qwen3-coder-next
        name: Qwen3 Coder Next (Bedrock)
        aliases:
          - qwen-next
          - qwen3-next
          - qwen-coder-next
        pricing:
          # Verified: AWS Bedrock (Feb 2026)
          input_per_million: 0.50
          output_per_million: 1.20
        inference_params:
          # Lowered from 0.7 for code review
          default_temperature: 0.3
          default_top_p: 0.95
          default_top_k: 40
          max_output_tokens: 16384
        context_window: 262000
        architecture: "MoE (80B total, 3B active)"
        capabilities:
          - code_generation
          - code_review
          - agentic_coding
          - tool_use
        notes: >-
          Ultra-sparse MoE (3B active),
          70%+ SWE-Bench Verified,
          purpose-built for coding agents

  # Azure OpenAI Provider
  azure_openai:
    endpoint: "${AZURE_OPENAI_ENDPOINT}"
    api_key: "${AZURE_OPENAI_API_KEY}"
    api_version: "2025-04-01-preview"
    models:
      # GPT-5.2 Codex Model
      # Best for: Enterprise code review
      # Note: Requires Responses API
      - id: gpt-5.2-codex
        deployment_name: gpt-5.2-codex
        name: GPT-5.2 Codex
        aliases:
          - gpt
          - gpt52
          - codex
        pricing:
          input_per_million: 1.75
          output_per_million: 14.00
          # 90% discount on cached
          cached_input_per_million: 0.175
        inference_params:
          # Zero temp for deterministic analysis
          default_temperature: 0.0
          default_top_p: 0.95
          max_output_tokens: 128000
        # GPT-5.2 Codex uses Responses API
        use_responses_api: true
        context_window: 400000
        capabilities:
          - reasoning
          - code_review
          - security_analysis
          - structured_output
          - function_calling
        notes: >-
          56.4% SWE-Bench Pro,
          security-aware by design

      # Kimi K2.5 (Azure AI Foundry)
      # MoE: 1T total, 32B active parameters
      # Best for: Reasoning, multimodal analysis
      - id: kimi-k2.5-azure
        deployment_name: Kimi-K2.5
        name: Kimi K2.5 (Azure)
        aliases:
          - kimi25-azure
          - kimi-azure
        pricing:
          # Microsoft Foundry (Feb 2026)
          input_per_million: 0.60
          output_per_million: 3.00
        inference_params:
          # Lowered from 0.6 for code review
          default_temperature: 0.5
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 262144
        capabilities:
          - reasoning
          - multimodal
          - code_review
          - tool_use
          - agentic_tasks
        notes: >-
          1T MoE multimodal, 96.1% AIME 2025,
          Agent Swarm support

      # Grok 4 Fast Reasoning (Azure Foundry)
      # xAI's cost-efficient reasoning model
      # Best for: Fast reasoning, cost-efficient
      - id: grok-4-fast
        deployment_name: grok-4-fast-reasoning
        name: Grok 4 Fast Reasoning (Azure)
        aliases:
          - grok
          - grok4
          - grok-fast
          - g4fast
        pricing:
          input_per_million: 0.20
          output_per_million: 0.50
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 32000
        context_window: 2000000
        capabilities:
          - reasoning
          - code_review
          - structured_output
          - function_calling
        notes: >-
          2M context, SOTA cost-efficiency,
          212 tokens/sec

  # NVIDIA NIM API Provider
  # Models available via build.nvidia.com
  nvidia:
    api_key: "${NVIDIA_API_KEY}"
    # base_url: "${NVIDIA_BASE_URL}"
    # 15 min for 202 polling
    polling_timeout: 900
    # Retry 504/502/503 up to 5 times
    max_retries: 5
    models:
      # Devstral 2 123B - Mistral code model
      # Best for: Code review, debugging
      # 72.2% SWE-Bench Verified score
      - id: devstral
        full_id: >-
          mistralai/devstral-2-123b-instruct-2512
        name: Devstral 2 123B
        aliases:
          - devstral-2
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          default_temperature: 0.15
          default_top_p: 0.95
          max_output_tokens: 16384
        context_window: 128000
        capabilities:
          - code_review
          - code_generation
          - debugging
          - agentic_coding
        notes: >-
          72.2% SWE-Bench Verified,
          excellent for code review tasks

      # MiniMax M2 (via NVIDIA)
      # Best for: Quick code analysis
      - id: minimax-nvidia
        full_id: minimaxai/minimax-m2
        name: MiniMax M2 (NVIDIA)
        aliases:
          - mm2-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lower for code review
          default_temperature: 0.3
          default_top_p: 0.9
          max_output_tokens: 16384
        context_window: 256000
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
        notes: >-
          Fast MoE model, good for
          quick code analysis

      # MiniMax M2.1 (via NVIDIA)
      # 200K context window, 128K max output
      # Best for: Code review, agentic workflows
      - id: minimax-m2.1-nvidia
        full_id: minimaxai/minimax-m2.1
        name: MiniMax M2.1 (NVIDIA)
        aliases:
          - mm2.1-nvidia
          - minimax-m2.1
          - mm21
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # MiniMax recommended
          default_temperature: 1.0
          default_top_p: 0.95
          default_top_k: 40
          max_output_tokens: 128000
          # Produces <think> reasoning tags
          enable_thinking: true
          clear_thinking: false
        context_window: 200000
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
          - code_review
          - thinking
        notes: >-
          Enhanced MoE with 200K context and
          128K output, coding/agentic optimized

      # MiniMax M2.5 (via NVIDIA)
      # MoE: 230B total, 10B active parameters
      # 204K shared context (input+output combined)
      # 80.2% SWE-Bench Verified, 37% faster than M2.1
      # Best for: Code review, agentic coding
      - id: minimax-m2.5-nvidia
        full_id: minimaxai/minimax-m2.5
        name: MiniMax M2.5 (NVIDIA)
        aliases:
          - mm2.5-nvidia
          - minimax-m2.5
          - mm25
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # IMPORTANT: Keep at 1.0 — M2.5 was RL-trained
          # assuming temp=1.0 (logits pass unscaled).
          # Lowering DEGRADES quality by distorting the
          # trained distribution. API rejects 0.0; valid
          # range is (0.0, 1.0]. Consistency is controlled
          # via top_p, not temperature.
          default_temperature: 1.0
          default_top_p: 0.95
          # top_k: stored for documentation but NOT passed
          # to NVIDIA NIM API (provider ignores it)
          default_top_k: 40
          # M2.5 supports up to 128K output natively.
          # NVIDIA NIM may cap lower (M2 had 16K limit);
          # reserves output space in token budget calc.
          max_output_tokens: 128000
          # Interleaved thinking (<think> tags) enables
          # deeper code analysis. Adds ~27-50% token
          # overhead to output but improves review quality.
          # Single-turn review — clear_thinking irrelevant.
          enable_thinking: true
          clear_thinking: false
        # Shared input+output window (confirmed: MiniMax
        # docs, NVIDIA model card, HuggingFace)
        context_window: 204800
        architecture: "MoE (230B total, 10B active)"
        capabilities:
          - coding
          - reasoning
          - agentic_tasks
          - code_review
          - thinking
          - tool_use
        notes: >-
          80.2% SWE-Bench Verified, SOTA coding
          and agentic tool use, 37% faster than
          M2.1, cost-efficient MoE

      # Qwen3 Coder 480B (via NVIDIA)
      # MoE: 480B total, 35B active parameters
      # Best for: Code generation, code review
      - id: qwen-nvidia
        full_id: >-
          qwen/qwen3-coder-480b-a35b-instruct
        name: Qwen3 Coder 480B (NVIDIA)
        aliases:
          - qwen3-nvidia
          - qwen-coder-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Matches Bedrock Qwen config
          default_temperature: 0.3
          default_top_p: 0.8
          default_top_k: 20
          # For detailed reports
          max_output_tokens: 16384
          # Deeper code analysis
          enable_thinking: true
          clear_thinking: false
        context_window: 131072
        architecture: "MoE (480B total, 35B active)"
        capabilities:
          - code_generation
          - code_review
          - agentic_coding
          - thinking
        notes: >-
          87.9% MultiPL-E, specialized
          for coding tasks via NVIDIA NIM

      # Qwen3.5 397B A17B (via NVIDIA)
      # MoE: 397B total, 17B active parameters
      # Best for: Code review, complex reasoning
      - id: qwen3.5-nvidia
        full_id: qwen/qwen3.5-397b-a17b
        name: Qwen3.5 397B A17B (NVIDIA)
        aliases:
          - qwen3.5
          - qwen35
          - qwen35-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lowered from 0.6 for code review
          default_temperature: 0.3
          default_top_p: 0.95
          default_top_k: 20
          # For detailed reports
          max_output_tokens: 16384
          # Enable thinking for reasoning
          enable_thinking: true
          clear_thinking: false
        context_window: 262144
        architecture: "MoE (397B total, 17B active)"
        capabilities:
          - code_generation
          - code_review
          - reasoning
          - thinking
        notes: >-
          Next-gen Qwen MoE with 262K context,
          thinking mode for complex analysis

      # DeepSeek V3.2 (via NVIDIA)
      # MoE: 671B total, 37B active parameters
      # Best for: Complex reasoning with thinking
      - id: deepseek-v3.2-nvidia
        full_id: deepseek-ai/deepseek-v3.2
        name: DeepSeek V3.2 (NVIDIA)
        aliases:
          - deepseek-v3-nvidia
          - ds-nvidia
          - deepseek-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lowered for code review
          default_temperature: 0.3
          default_top_p: 0.9
          max_output_tokens: 16384
          # Deep reasoning mode
          enable_thinking: true
          clear_thinking: false
        context_window: 128000
        architecture: "MoE (671B total, 37B active)"
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - thinking
        notes: >-
          Advanced reasoning with thinking,
          excellent for complex code analysis

      # Kimi K2.5 (via NVIDIA)
      # MoE: 1T total, multimodal
      # Best for: Reasoning, multimodal analysis
      - id: kimi-k2.5-nvidia
        full_id: moonshotai/kimi-k2.5
        name: Kimi K2.5 (NVIDIA)
        aliases:
          - kimi-k2.5
          - kimi25
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lowered from 0.6 for code review
          default_temperature: 0.5
          default_top_p: 0.95
          default_top_k: 40
          # Up to 32K supported
          max_output_tokens: 32768
        context_window: 256000
        architecture: "MoE (1T total, multimodal)"
        capabilities:
          - reasoning
          - multimodal
          - code_review
          - tool_use
        notes: >-
          1T multimodal MoE for video and
          image understanding, efficient

      # GLM4.7 (via NVIDIA) - Zhipu AI
      # 73.8% SWE-bench, interleaved thinking
      # Best for: Complex reasoning with thinking
      - id: glm47
        full_id: z-ai/glm4.7
        name: GLM 4.7 (NVIDIA)
        aliases:
          - glm4
          - glm-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Default 1.0, SWE-bench uses 0.7
          default_temperature: 0.5
          default_top_p: 0.95
          # SWE-bench setting
          max_output_tokens: 16384
          # Interleaved thinking
          enable_thinking: true
          clear_thinking: false
        context_window: 200000
        capabilities:
          - reasoning
          - code_review
          - thinking
          - tool_use
        notes: >-
          73.8% SWE-bench, interleaved thinking
          for complex code analysis

      # GLM-5 (via NVIDIA) - Zhipu AI next-gen
      # 200K context window, 128K max output
      # Best for: Complex reasoning, general AI
      - id: glm5
        full_id: z-ai/glm5
        name: GLM-5 (NVIDIA)
        aliases:
          - glm-5
          - glm5-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lowered from 1.0 per model docs:
          # use 0.5-0.7 for non-creative prompts
          default_temperature: 0.5
          default_top_p: 0.95
          max_output_tokens: 128000
        context_window: 200000
        capabilities:
          - reasoning
          - code_review
          - code_generation
          - general_intelligence
        notes: >-
          Next-gen GLM, 200K context, 128K
          output.

      # Step-3.5-Flash (via NVIDIA) - StepFun
      # 256K context, optimized for fast inference
      # Best for: General code review, agentic
      - id: step-3.5-flash
        full_id: stepfun-ai/step-3.5-flash
        name: Step 3.5 Flash (NVIDIA)
        aliases:
          - step-flash
          - step35
          - step-nvidia
        pricing:
          input_per_million: 0.00
          output_per_million: 0.00
        inference_params:
          # Lowered from 0.6 for code review
          default_temperature: 0.3
          default_top_p: 0.95
          default_top_k: 40
          # For detailed reports
          max_output_tokens: 16384
        context_window: 256000
        capabilities:
          - code_review
          - reasoning
          - general_intelligence
        notes: >-
          Cost-efficient 256K context model,
          good coherence at temperature 0.6

  # Google Generative AI Provider
  # Models via Google AI Studio / Gemini API
  google_genai:
    api_key: "${GOOGLE_API_KEY}"
    models:
      # Gemini 3 Pro Preview - Google flagship
      # 1M token context, strong reasoning
      - id: gemini-3-pro
        full_id: gemini-3-pro-preview
        name: Gemini 3 Pro Preview
        aliases:
          - gemini-pro
          - gemini3-pro
          - g3pro
        pricing:
          input_per_million: 2.00
          output_per_million: 12.00
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 1000000
        capabilities:
          - code_review
          - reasoning
          - multimodal
          - tool_use

      # Gemini 3.1 Pro Preview - Most advanced reasoning
      # 1M token context, Deep Think with 3 levels
      # 77.1% ARC-AGI-2, 94.3% GPQA Diamond
      - id: gemini-3.1-pro
        full_id: gemini-3.1-pro-preview
        name: Gemini 3.1 Pro Preview
        aliases:
          - gemini31-pro
          - g31pro
        pricing:
          input_per_million: 2.00
          output_per_million: 12.00
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 1000000
        capabilities:
          - code_review
          - reasoning
          - multimodal
          - tool_use
          - agentic_tasks
        notes: >-
          77.1% ARC-AGI-2, 94.3% GPQA Diamond,
          2x reasoning over Gemini 3 Pro,
          Deep Think with 3 levels

      # Gemini 3 Flash Preview - Fast, efficient
      # 1M token context, Flash-tier pricing
      - id: gemini-3-flash
        full_id: gemini-3-flash-preview
        name: Gemini 3 Flash Preview
        aliases:
          - gemini-flash
          - gemini3-flash
          - g3flash
        pricing:
          input_per_million: 0.50
          output_per_million: 3.00
        inference_params:
          default_temperature: 0.1
          default_top_p: 0.95
          max_output_tokens: 65536
        context_window: 1000000
        capabilities:
          - code_review
          - reasoning
          - multimodal

# Default model configuration
defaults:
  bedrock_default: opus
  azure_default: gpt-5.2-codex
  nvidia_default: devstral
  aws_region: us-west-2
  max_tokens: 16000

# File scanning configuration
scanning:
  max_file_size_kb: 500
  warn_file_size_kb: 100
  exclude_patterns:
    # Dependency directories
    - "**/node_modules/**"
    - "**/.venv/**"
    - "**/venv/**"
    - "**/vendor/**"
    # Build outputs
    - "**/dist/**"
    - "**/build/**"
    - "**/__pycache__/**"
    - "**/*.pyc"
    - "**/*.pyo"
    # Version control
    - "**/.git/**"
    - "**/.svn/**"
    # IDE and editors
    - "**/.vscode/**"
    - "**/.idea/**"
    - "**/*.swp"
    # Test fixtures and generated code
    - "**/test_data/**"
    - "**/fixtures/**"
    - "**/*_pb2.py"
    - "**/*_pb2_grpc.py"
    # Common non-review files
    - "**/*.min.js"
    - "**/*.min.css"
    - "**/migrations/**"
    - "**/*.lock"
  exclude_extensions:
    - ".json"
    - ".yaml"
    - ".yml"
    - ".toml"
    - ".md"
    - ".txt"
    - ".rst"
    - ".jpg"
    - ".png"
    - ".gif"
    - ".svg"
    - ".bin"
    - ".exe"
    - ".so"
    - ".dylib"
    - ".pyc"
    - ".pyo"

# Code Review Recommendations
#
# For accurate, deterministic code review:
# - Use temperature 0.0-0.3 (lower = consistent)
# - Use top_p 0.8-0.95 for controlled output
# - Prefer models with strong SWE-bench scores
#
# Model selection by use case:
# - Quick: Haiku ($1/$5) or Qwen ($0.22/$1.40)
# - Standard: Sonnet 4.6 ($3/$15) or Gemini ($2/$12)
# - Complex: Opus ($5/$25) or GPT-5.2 ($1.75/$14)
# - Budget: Qwen ($0.22/$1.40) or Grok ($0.20)
